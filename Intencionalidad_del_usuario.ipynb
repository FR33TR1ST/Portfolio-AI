{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2d85eb-a583-4ab6-858f-4646db603c27",
   "metadata": {},
   "source": [
    "# Clasificacion de intencionalidad del usuario\n",
    "El proyecto consta en clasificar que esta intentando realizar el usuario utilizando Procesamiento de Lenguaje Natural, Transformers y Torch.\n",
    "\n",
    "Durante el proyecto se llego a la conclusion de que el clasificador dio como resultado un 98% de asertividad (accuracy) por lo que clasifica bien 98 de cada 100 frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e27fd28-6771-4fca-855a-c1d9f3b65047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cc6253-7100-49ef-bef5-f12404461964",
   "metadata": {},
   "source": [
    "# Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6b089f-f7d9-46fe-bd39-d579dc8deefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:402: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 00:20, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.848000</td>\n",
       "      <td>2.848773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.845400</td>\n",
       "      <td>2.843713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.832500</td>\n",
       "      <td>2.835877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.828000</td>\n",
       "      <td>2.822277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.807300</td>\n",
       "      <td>2.799058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.794600</td>\n",
       "      <td>2.768227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.747800</td>\n",
       "      <td>2.716010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.715000</td>\n",
       "      <td>2.645125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.641200</td>\n",
       "      <td>2.550757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>2.442733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.424800</td>\n",
       "      <td>2.324721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.279000</td>\n",
       "      <td>2.189169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.139400</td>\n",
       "      <td>2.058049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.098200</td>\n",
       "      <td>1.914267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.932800</td>\n",
       "      <td>1.747173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.739900</td>\n",
       "      <td>1.576354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>1.402710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.391200</td>\n",
       "      <td>1.237431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.285400</td>\n",
       "      <td>1.077890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.049700</td>\n",
       "      <td>0.926220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.965200</td>\n",
       "      <td>0.799156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.804600</td>\n",
       "      <td>0.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.678300</td>\n",
       "      <td>0.567122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.599600</td>\n",
       "      <td>0.461165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.517400</td>\n",
       "      <td>0.381831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.417100</td>\n",
       "      <td>0.310327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.256523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.300100</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.91%\n",
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "       PlayMusic       1.00      1.00      1.00        13\n",
      "  IncreaseVolume       1.00      1.00      1.00        21\n",
      "      GetWeather       1.00      1.00      1.00        12\n",
      "            Time       1.00      1.00      1.00        23\n",
      "    TurnOnLights       1.00      1.00      1.00        28\n",
      "  InternetSearch       0.87      1.00      0.93        46\n",
      "            Maps       1.00      0.76      0.86        25\n",
      "  BookRestaurant       1.00      1.00      1.00        32\n",
      "   YouTubeVideos       1.00      0.95      0.97        19\n",
      "  DecreaseVolume       1.00      0.95      0.97        20\n",
      "      MuteVolume       0.92      1.00      0.96        11\n",
      "       SetVolume       1.00      1.00      1.00        26\n",
      "   YoutubeVideos       1.00      1.00      1.00        22\n",
      "       Translate       1.00      1.00      1.00        21\n",
      "StockMarketQuery       1.00      1.00      1.00        18\n",
      "      Initialize       1.00      1.00      1.00        20\n",
      "            Joke       1.00      1.00      1.00        25\n",
      "\n",
      "        accuracy                           0.98       382\n",
      "       macro avg       0.99      0.98      0.98       382\n",
      "    weighted avg       0.98      0.98      0.98       382\n",
      "\n",
      "\n",
      "Comparison between real and predicted intents:\n",
      "                                               Sentence       Real Intent  \\\n",
      "1226                 Can you raise the volume by a bit?    IncreaseVolume   \n",
      "111      Show me directions to the nearest coffee shop.              Maps   \n",
      "1839                         Can you share a one-liner?              Joke   \n",
      "1675        Show me the latest news on Coca-Cola stock.  StockMarketQuery   \n",
      "415    Find a restaurant that serves vegetarian dishes.    BookRestaurant   \n",
      "1366         Can you turn on the lights in the hallway?      TurnOnLights   \n",
      "70    Set a reminder for my dentist appointment at 3...              Time   \n",
      "530       Find the quickest way to my office from here.              Maps   \n",
      "617          What are some popular social media trends?    InternetSearch   \n",
      "1569                  Translate 'thank you' to Spanish.         Translate   \n",
      "\n",
      "      Predicted Intent  \n",
      "1226    IncreaseVolume  \n",
      "111               Maps  \n",
      "1839              Joke  \n",
      "1675  StockMarketQuery  \n",
      "415     BookRestaurant  \n",
      "1366      TurnOnLights  \n",
      "70                Time  \n",
      "530               Maps  \n",
      "617     InternetSearch  \n",
      "1569         Translate  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Datasets_NLU/Dataset_final_with_joke.csv')\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Sentence'], df['Intent'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenizer for DistilBERT\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Tokenize the dataset\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert labels to tensors\n",
    "intent_labels = df['Intent'].unique()\n",
    "intent_label_map = {label: idx for idx, label in enumerate(intent_labels)}\n",
    "\n",
    "train_labels = [intent_label_map[intent] for intent in y_train]\n",
    "test_labels = [intent_label_map[intent] for intent in y_test]\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Dataset class for PyTorch\n",
    "class NLU_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = NLU_Dataset(train_encodings, train_labels)\n",
    "test_dataset = NLU_Dataset(test_encodings, test_labels)\n",
    "\n",
    "# Load the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(intent_labels))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor before using argmax\n",
    "preds = torch.tensor(predictions.predictions).argmax(axis=-1)\n",
    "\n",
    "# Convert predicted labels back to intention names\n",
    "predicted_intents = [intent_labels[pred] for pred in preds]\n",
    "\n",
    "# Compare predicted intents with real intents\n",
    "real_intents = [intent_label_map[intent] for intent in y_test]  # Convert real intents to numeric values\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(real_intents, preds)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Show classification report (Precision, Recall, F1-Score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(real_intents, preds, target_names=intent_labels))\n",
    "\n",
    "# Optional: Display comparison between predicted and real intents\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Sentence': X_test,\n",
    "    'Real Intent': y_test,\n",
    "    'Predicted Intent': predicted_intents\n",
    "})\n",
    "print(\"\\nComparison between real and predicted intents:\")\n",
    "print(comparison_df.head(10))  # Display the first 10 comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "021e4d2a-a0c0-4e0d-a6d8-0243dba3ce78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intent\n",
       "InternetSearch      189\n",
       "Time                147\n",
       "SetVolume           137\n",
       "YouTubeVideos       129\n",
       "BookRestaurant      123\n",
       "YoutubeVideos       112\n",
       "MuteVolume          106\n",
       "IncreaseVolume      105\n",
       "DecreaseVolume      105\n",
       "TurnOnLights        104\n",
       "Maps                104\n",
       "StockMarketQuery    100\n",
       "Initialize          100\n",
       "Joke                100\n",
       "Translate           100\n",
       "GetWeather           73\n",
       "PlayMusic            72\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Intent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4cd57b-3b9e-4ba7-9fc5-71ad3a24b7dd",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7b41c6-dc0c-4903-ac23-df675abd4a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_trf' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\thinc\\shims\\pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Play Barbie Girl, search Coffe, and increase the volume.\n",
      "Extracted Intentions:\n",
      "- Play Barbie Girl ,\n",
      "- search Coffe , and\n",
      "- increase the volume .\n",
      "--------------------------------------------------\n",
      "Sentence: Play music, turn off the lights, and lock the doors.\n",
      "Extracted Intentions:\n",
      "- Play music ,\n",
      "- turn off the lights , and\n",
      "- lock the doors .\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy Transformer-based English model\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "# Set of action words (lemmas)\n",
    "action_words = set([\n",
    "    'play', 'open', 'book', 'set', 'turn', 'call', 'email', 'buy', 'cook', 'send',\n",
    "    'increase', 'decrease', 'mute', 'lock', 'unlock', 'remind', 'wake', 'schedule',\n",
    "    'watch', 'read', 'write', 'summarize', 'submit', 'practice', 'message',\n",
    "    'update', 'listen', 'find', 'search', 'get', 'make', 'give', 'take', 'tell',\n",
    "    'ask', 'work', 'go', 'do', 'be', 'have', 'prepare', 'pack', 'check', 'respond',\n",
    "    'delete', 'start', 'stop', 'pause', 'resume', 'drive', 'park', 'notify',\n",
    "    'forget', 'order', 'share', 'light', 'close', 'strike', 'continue', 'begin',\n",
    "    'finish', 'analyze', 'proceed', 'monitor', 'log', 'facilitate', 'engage',\n",
    "    'optimize', 'deploy', 'research', 'compile', 'pick', 'drop', 'wrap', 'meet',\n",
    "    'discuss', 'inform', 'shut', 'put', 'connect', 'disconnect', 'install', 'uninstall',\n",
    "    'activate', 'deactivate', 'adjust', 'clear', 'show', 'hide', 'track', 'follow',\n",
    "    'block', 'unblock', 'build', 'fix', 'draw', 'print', 'edit', 'clip', 'crop',\n",
    "    'zoom', 'upload', 'download', 'record', 'stream', 'playback', 'charge', 'pay',\n",
    "    'deliver', 'explore', 'design', 'test', 'help', 'assist', 'update', 'organize',\n",
    "    'remind', 'plan', 'recommend', 'subscribe', 'unsubscribe', 'post', 'comment',\n",
    "    'like', 'dislike', 'tag', 'mention', 'call', 'text', 'scan', 'encrypt', 'decrypt',\n",
    "    'sign', 'authenticate', 'scan', 'record', 'repeat', 'shuffle', 'translate'\n",
    "])\n",
    "\n",
    "\n",
    "def extract_intentions(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    intentions = []\n",
    "    current_intent = []\n",
    "\n",
    "    for token in doc:\n",
    "        # Identify if the token is an action verb or auxiliary\n",
    "        is_action = (\n",
    "            token.lemma_.lower() in action_words and\n",
    "            token.pos_ in ('VERB', 'AUX')\n",
    "        )\n",
    "\n",
    "        if is_action:\n",
    "            # If we encounter a new action and have accumulated tokens, store the current intention\n",
    "            if current_intent:\n",
    "                intentions.append(' '.join(tok.text for tok in current_intent).strip())\n",
    "                current_intent = []\n",
    "\n",
    "        # Accumulate tokens for the current intention\n",
    "        current_intent.append(token)\n",
    "\n",
    "        # Handle conjunctions (e.g., 'and', 'or') connecting different actions\n",
    "        if token.dep_ == 'cc' and current_intent:\n",
    "            intentions.append(' '.join(tok.text for tok in current_intent).strip())\n",
    "            current_intent = []\n",
    "\n",
    "    # Add the final intention if it exists\n",
    "    if current_intent:\n",
    "        intentions.append(' '.join(tok.text for tok in current_intent).strip())\n",
    "\n",
    "    return intentions\n",
    "\n",
    "# Test sentences\n",
    "if __name__ == \"__main__\":\n",
    "    sentences = [\n",
    "        \"Play Barbie Girl, search Coffe, and increase the volume.\",\n",
    "        \"Play music, turn off the lights, and lock the doors.\",\n",
    "    ]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        intentions = extract_intentions(sentence)\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(\"Extracted Intentions:\")\n",
    "        for intent in intentions:\n",
    "            print(f\"- {intent}\")\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397f62a-0188-47bb-bf72-4a81c6fa0ee2",
   "metadata": {},
   "source": [
    "# Preprocessing and Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6b95bae-89d3-46a2-8e8c-0c22b997ec70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>play taylor swift and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>increase the volume</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sentences\n",
       "0  play taylor swift and\n",
       "1    increase the volume"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_predict = extract_intentions('play taylor swift and increase the volume')\n",
    "df_sentences = pd.DataFrame(sentences_predict, columns=['sentences'])\n",
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed77e2a2-0c85-43bd-a4b2-07ec7f49e4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_sentences_encodings = tokenizer(list(df_sentences['sentences']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create a dataset using the same class\n",
    "new_sentences_dataset = NLU_Dataset(new_sentences_encodings, torch.tensor([0]*len(df_sentences)))  # Labels are dummy values\n",
    "\n",
    "sentence_predicted = trainer.predict(new_sentences_dataset)\n",
    "\n",
    "# Predict on the new sentences\n",
    "new_predictions = trainer.predict(new_sentences_dataset)\n",
    "\n",
    "# Convert the NumPy array to a PyTorch tensor and get the predicted intents\n",
    "new_preds = torch.tensor(new_predictions.predictions).argmax(axis=-1)\n",
    "\n",
    "# Convert predicted labels back to intention names\n",
    "predicted_new_intents = [intent_labels[pred] for pred in new_preds]\n",
    "\n",
    "# Show the predictions\n",
    "df_sentences['Predicted Intent'] = predicted_new_intents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d26cc2-1f99-4ed6-8bdb-aed71cdd5ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>Predicted Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>play taylor swift and</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>increase the volume</td>\n",
       "      <td>IncreaseVolume</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sentences Predicted Intent\n",
       "0  play taylor swift and        PlayMusic\n",
       "1    increase the volume   IncreaseVolume"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57261c2b-ab99-422c-b315-3155d982abe1",
   "metadata": {},
   "source": [
    "# Split and Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656d876a-ed3d-4adf-8171-dac04fca1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_categorize(user_input):\n",
    "    sentences_predict = extract_intentions(user_input)\n",
    "    df_sentences = pd.DataFrame(sentences_predict, columns=['sentences'])\n",
    "    new_sentences_encodings = tokenizer(list(df_sentences['sentences']), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "    # Create a dataset using the same class\n",
    "    new_sentences_dataset = NLU_Dataset(new_sentences_encodings, torch.tensor([0]*len(df_sentences)))  # Labels are dummy values\n",
    "    \n",
    "    sentence_predicted = trainer.predict(new_sentences_dataset)\n",
    "    \n",
    "    # Predict on the new sentences\n",
    "    new_predictions = trainer.predict(new_sentences_dataset)\n",
    "    \n",
    "    # Convert the NumPy array to a PyTorch tensor and get the predicted intents\n",
    "    new_preds = torch.tensor(new_predictions.predictions).argmax(axis=-1)\n",
    "    \n",
    "    # Convert predicted labels back to intention names\n",
    "    predicted_new_intents = [intent_labels[pred] for pred in new_preds]\n",
    "    \n",
    "    # Show the predictions\n",
    "    df_sentences['Predicted Intent'] = predicted_new_intents\n",
    "    return df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8c2115b-52b8-4043-9e88-e71db9d28669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaspa\\OneDrive\\Escritorio\\Facu\\venv\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>Predicted Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shut up the video</td>\n",
       "      <td>MuteVolume</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentences Predicted Intent\n",
       "0  shut up the video       MuteVolume"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorized_sentences = split_and_categorize('shut up the video')\n",
    "categorized_sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d49ece-e23e-4048-9005-8a88ec7bbfdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
